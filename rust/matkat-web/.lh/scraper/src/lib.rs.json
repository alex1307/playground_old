{
    "sourceFile": "scraper/src/lib.rs",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 92,
            "patches": [
                {
                    "date": 1641556596513,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1641560301917,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,257 @@\n+pub mod cambridge_scraper;\n+pub mod collins_scraper;\n+pub mod oxford_scraper;\n+pub mod unit_tests;\n+use std::io::Cursor;\n+\n+use crate::{\n+    domain::{\n+        error::{DictionaryError, DictionaryErrorKind, ServiceError, ServiceErrorType},\n+        Dictionary, DictionaryEntry, ServiceExuctionResult,\n+    },\n+    files::read_file_content,\n+    utils::{replace_tabs, trim_tabs},\n+    MP3_EXT,\n+};\n+use itertools::Itertools;\n+use log::{debug, error, info};\n+use reqwest::StatusCode;\n+use scraper::{Html, Selector};\n+\n+const INVALID: &str = \"invalid\";\n+const SEARCH_FOR: &str = \"search suggestions for\";\n+const NO_RESULTS: &str = \"sorry, no results for\";\n+const PAGE_NOT_FOUND: &str = \"page not found\";\n+const BROWSE_DICTIONARY: &str = \"browse the english dictionary\";\n+const NOT_FOUND: [&str; 4] = [NO_RESULTS, PAGE_NOT_FOUND, SEARCH_FOR, BROWSE_DICTIONARY];\n+\n+pub fn wget(url: &str, file_name: &str) -> ServiceExuctionResult<String> {\n+    let status = std::process::Command::new(\"wget\")\n+    .arg(\"--user-agent\")\n+    .arg(\"'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15'\")\n+    .arg(url)\n+    .arg(\"-O\")\n+    .arg(file_name)\n+    .status();\n+\n+    if status.is_err() {\n+        error!(\"failed.executing.wget: {}\", status.err().unwrap());\n+        return Err(ServiceError {\n+            message: \"wget.failed\".to_string(),\n+            error_type: ServiceErrorType::Failure,\n+        });\n+    }\n+\n+    let bytes = read_file_content(file_name)?;\n+    Ok(String::from_utf8(bytes)?)\n+}\n+\n+pub async fn fetch_url(url: String, file_name: String) -> ServiceExuctionResult<()> {\n+    debug!(\"downloading from [{}] to {}\", url, file_name);\n+    let response = reqwest::get(url.clone()).await.unwrap();\n+    let status = response.status();\n+    info!(\"download status: {}\", status);\n+    if status.is_success() {\n+        let mut file = std::fs::File::create(file_name.clone())?;\n+        let mut content = Cursor::new(response.bytes().await?);\n+        std::io::copy(&mut content, &mut file)?;\n+        Ok(())\n+    } else {\n+        match wget(&url, &file_name) {\n+            Ok(_) => Ok(()),\n+            Err(err) => Err(err)\n+        }\n+    }\n+    \n+}\n+\n+async fn download_from_url(url: &str) -> ServiceExuctionResult<String> {\n+    println!(\"URL: {}\", url.clone());\n+    let response = reqwest::get(url).await?;\n+    println!(\"resposne: {:?}\", response);\n+    if StatusCode::OK != response.status() {\n+        let msg = format!(\"nok.status.code: {}\", response.status().as_str());\n+        return Err(ServiceError {\n+            message: msg,\n+            error_type: ServiceErrorType::Unavailable,\n+        });\n+    }\n+\n+    let bytes = response.bytes().await.unwrap();\n+    let slice = &bytes[..];\n+    let content = String::from_utf8(Vec::from(slice)).unwrap();\n+    let lower = content.to_lowercase();\n+    for i in 0..NOT_FOUND.len() {\n+        if lower.contains(NOT_FOUND[i]) {\n+            return Err(ServiceError {\n+                message: \"not.found\".to_string(),\n+                error_type: ServiceErrorType::ResourceNotFound,\n+            });\n+        }\n+    }\n+    Ok(content)\n+}\n+\n+fn description(inner_html: String) -> Option<String> {\n+    let mut replaced = match inner_text(inner_html.clone(), \"a\") {\n+        Ok(inner) => inner,\n+        Err(_err) => return None,\n+    };\n+\n+    replaced = match inner_text(replaced, \"span\") {\n+        Ok(inner) => inner,\n+        Err(_) => return None,\n+    };\n+\n+    replaced = replaced.replace(\"\\n\", \" \");\n+    replaced = trim_tabs(&mut replaced);\n+    replaced = replaced.replace(\" .\", \".\");\n+    Some(replaced)\n+}\n+\n+fn inner_text(inner_html: String, tag: &str) -> Result<String, DictionaryError> {\n+    let mut replaced = inner_html.clone();\n+    let mut open_tag = String::from(\"<\");\n+    open_tag.push_str(tag);\n+    let mut close_tag = String::from(\"</\");\n+    close_tag.push_str(tag);\n+    close_tag.push_str(\">\");\n+\n+    if !(inner_html.contains(&open_tag) && inner_html.contains(&close_tag)) {\n+        return Ok(inner_html);\n+    }\n+\n+    let tag_selector = Selector::parse(tag).unwrap();\n+\n+    while replaced.contains(&open_tag) && replaced.contains(&close_tag) {\n+        let start = replaced.find(&open_tag).unwrap();\n+        let end = replaced.find(&close_tag).unwrap();\n+        if start < end + close_tag.len() {\n+            let to_be_replaced = &replaced[start..end + close_tag.len()];\n+            let a_href = Html::parse_fragment(&replaced);\n+            let inner = a_href.select(&tag_selector).next().unwrap().inner_html();\n+            replaced = replaced.replace(to_be_replaced, &inner);\n+        } else {\n+            error!(\n+                \"parse.error: start postion {} is bofore end: {}\",\n+                start,\n+                end + close_tag.len()\n+            );\n+            return Err(DictionaryError::throw(\n+                \"unable.to.parse.definition\",\n+                DictionaryErrorKind::InvalidData,\n+            ));\n+        }\n+    }\n+    Ok(replaced)\n+}\n+\n+fn elements(query: &str, html_content: &str, inner: bool) -> Vec<String> {\n+    let fragment = Html::parse_fragment(&html_content);\n+    let selector = Selector::parse(query).unwrap();\n+    let elements = fragment.select(&selector).into_iter();\n+    let mut all = Vec::new();\n+    for e in elements {\n+        if inner {\n+            all.push(e.inner_html());\n+        } else {\n+            all.push(e.html());\n+        }\n+    }\n+    all\n+}\n+\n+fn first_element(query: &str, html_content: &str, inner: bool) -> Option<String> {\n+    let fragment = Html::parse_fragment(&html_content);\n+    let selector = Selector::parse(query).unwrap();\n+    let mut elements = fragment.select(&selector).into_iter();\n+    let found = elements.next();\n+\n+    if found.is_some() {\n+        return if inner {\n+            Some(found.unwrap().inner_html())\n+        } else {\n+            Some(found.unwrap().html())\n+        };\n+    }\n+\n+    None\n+}\n+\n+fn mp3_element(mp3_query: &str, html_content: &str) -> Option<String> {\n+    first_element(mp3_query, html_content, false)\n+}\n+\n+pub async fn merge_definitions(url1: &str, url2: &str, url3: &str) -> Option<DictionaryEntry> {\n+    let source_1 = oxford_scraper::download(url1).await;\n+    let source_2 = oxford_scraper::download(url2).await;\n+\n+    if source_1.is_err() {\n+        return None;\n+    }\n+\n+    if source_2.is_err() {\n+        return None;\n+    }\n+\n+    let mut merged = source_1.unwrap();\n+    for def in source_2.unwrap().definitions {\n+        merged.definitions.push(def);\n+    }\n+    if !url3.is_empty() {\n+        let source_3 = oxford_scraper::download(url3).await;\n+        if source_3.is_ok() {\n+            for def in source_3.unwrap().definitions {\n+                merged.definitions.push(def);\n+            }\n+        }\n+    }\n+\n+    Some(merged)\n+}\n+\n+fn to_url(base_url: &str, word: &str) -> String {\n+    if word.is_empty() {\n+        return INVALID.to_lowercase();\n+    }\n+\n+    if base_url.is_empty() {\n+        return INVALID.to_lowercase();\n+    }\n+    let mut w = String::from(word);\n+    let mut trimmed = trim_tabs(&mut w);\n+    let processed = replace_tabs(&mut trimmed, \"-\");\n+    let mut url = String::from(base_url);\n+    url.push_str(&processed);\n+    url.to_lowercase()\n+}\n+\n+fn mp3_element_to_url(source: &str, start_with: &str) -> Option<String> {\n+    element_to_url(source, start_with, MP3_EXT)\n+}\n+\n+fn element_to_url(source: &str, start_with: &str, end_with: &str) -> Option<String> {\n+    let values: Vec<&str> = source.split_whitespace().collect();\n+    let found = values\n+        .into_iter()\n+        .find_or_first(|s| s.contains(start_with) && s.contains(end_with));\n+\n+    let raw_element = if found.is_some() {\n+        found.unwrap()\n+    } else {\n+        return None;\n+    };\n+\n+    let start = raw_element.find(start_with).unwrap();\n+    let end = raw_element.find(end_with).unwrap() + end_with.len();\n+    Some(raw_element[start..end].to_lowercase())\n+}\n+\n+pub async fn scrape_it_from(dictionary: String, word: &str) -> Option<DictionaryEntry> {\n+    match Dictionary::from(dictionary) {\n+        Dictionary::Cambridge => cambridge_scraper::scrape(word).await,\n+        Dictionary::Collins => collins_scraper::scrape(word).await,\n+        Dictionary::Oxford => oxford_scraper::scrape(word).await,\n+        _ => return None,\n+    }\n+}\n"
                },
                {
                    "date": 1641794469258,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,12 +4,8 @@\n pub mod unit_tests;\n use std::io::Cursor;\n \n use crate::{\n-    domain::{\n-        error::{DictionaryError, DictionaryErrorKind, ServiceError, ServiceErrorType},\n-        Dictionary, DictionaryEntry, ServiceExuctionResult,\n-    },\n     files::read_file_content,\n     utils::{replace_tabs, trim_tabs},\n     MP3_EXT,\n };\n"
                },
                {
                    "date": 1641794479344,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,8 +8,9 @@\n     files::read_file_content,\n     utils::{replace_tabs, trim_tabs},\n     MP3_EXT,\n };\n+use common_libs::error::ServiceExuctionResult;\n use itertools::Itertools;\n use log::{debug, error, info};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n"
                },
                {
                    "date": 1641794487601,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n     files::read_file_content,\n     utils::{replace_tabs, trim_tabs},\n     MP3_EXT,\n };\n-use common_libs::error::ServiceExuctionResult;\n+use common_libs::error::{ServiceExuctionResult, ServiceError};\n use itertools::Itertools;\n use log::{debug, error, info};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n"
                },
                {
                    "date": 1641795086546,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,16 +1,17 @@\n pub mod cambridge_scraper;\n pub mod collins_scraper;\n pub mod oxford_scraper;\n pub mod unit_tests;\n+pub mod const;\n use std::io::Cursor;\n \n use crate::{\n     files::read_file_content,\n     utils::{replace_tabs, trim_tabs},\n     MP3_EXT,\n };\n-use common_libs::error::{ServiceExuctionResult, ServiceError};\n+use common_libs::error::{ServiceExuctionResult, ServiceError, ServiceErrorType};\n use itertools::Itertools;\n use log::{debug, error, info};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n"
                },
                {
                    "date": 1641795118161,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,9 @@\n pub mod cambridge_scraper;\n pub mod collins_scraper;\n pub mod oxford_scraper;\n pub mod unit_tests;\n-pub mod const;\n+pub mod constants;\n use std::io::Cursor;\n \n use crate::{\n     files::read_file_content,\n"
                },
                {
                    "date": 1641795216558,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,19 +15,13 @@\n use log::{debug, error, info};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n \n-const INVALID: &str = \"invalid\";\n-const SEARCH_FOR: &str = \"search suggestions for\";\n-const NO_RESULTS: &str = \"sorry, no results for\";\n-const PAGE_NOT_FOUND: &str = \"page not found\";\n-const BROWSE_DICTIONARY: &str = \"browse the english dictionary\";\n-const NOT_FOUND: [&str; 4] = [NO_RESULTS, PAGE_NOT_FOUND, SEARCH_FOR, BROWSE_DICTIONARY];\n \n pub fn wget(url: &str, file_name: &str) -> ServiceExuctionResult<String> {\n     let status = std::process::Command::new(\"wget\")\n     .arg(\"--user-agent\")\n-    .arg(\"'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15'\")\n+    .arg(USER_AGENT)\n     .arg(url)\n     .arg(\"-O\")\n     .arg(file_name)\n     .status();\n"
                },
                {
                    "date": 1641795281179,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,12 +6,12 @@\n use std::io::Cursor;\n \n use crate::{\n     files::read_file_content,\n-    utils::{replace_tabs, trim_tabs},\n-    MP3_EXT,\n+    utils::{replace_tabs, trim_tabs}\n };\n use common_libs::error::{ServiceExuctionResult, ServiceError, ServiceErrorType};\n+use constants::USER_AGENT;\n use itertools::Itertools;\n use log::{debug, error, info};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n"
                },
                {
                    "date": 1641795296917,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,12 +4,9 @@\n pub mod unit_tests;\n pub mod constants;\n use std::io::Cursor;\n \n-use crate::{\n-    files::read_file_content,\n-    utils::{replace_tabs, trim_tabs}\n-};\n+\n use common_libs::error::{ServiceExuctionResult, ServiceError, ServiceErrorType};\n use constants::USER_AGENT;\n use itertools::Itertools;\n use log::{debug, error, info};\n"
                },
                {
                    "date": 1641795351391,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n pub mod constants;\n use std::io::Cursor;\n \n \n-use common_libs::error::{ServiceExuctionResult, ServiceError, ServiceErrorType};\n+use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType}, files::read_file_content};\n use constants::USER_AGENT;\n use itertools::Itertools;\n use log::{debug, error, info};\n use reqwest::StatusCode;\n"
                },
                {
                    "date": 1641795411361,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,11 @@\n use log::{debug, error, info};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n \n+use crate::constants::NOT_FOUND;\n \n+\n pub fn wget(url: &str, file_name: &str) -> ServiceExuctionResult<String> {\n     let status = std::process::Command::new(\"wget\")\n     .arg(\"--user-agent\")\n     .arg(USER_AGENT)\n"
                },
                {
                    "date": 1641795423298,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n pub mod constants;\n use std::io::Cursor;\n \n \n-use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType}, files::read_file_content};\n+use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType}, files::read_file_content, utils::trim_tabs};\n use constants::USER_AGENT;\n use itertools::Itertools;\n use log::{debug, error, info};\n use reqwest::StatusCode;\n"
                },
                {
                    "date": 1641795463621,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n pub mod constants;\n use std::io::Cursor;\n \n \n-use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType}, files::read_file_content, utils::trim_tabs};\n+use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType, DictionaryError, DictionaryErrorKind}, files::read_file_content, utils::trim_tabs};\n use constants::USER_AGENT;\n use itertools::Itertools;\n use log::{debug, error, info};\n use reqwest::StatusCode;\n"
                },
                {
                    "date": 1641795482666,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -172,9 +172,9 @@\n fn mp3_element(mp3_query: &str, html_content: &str) -> Option<String> {\n     first_element(mp3_query, html_content, false)\n }\n \n-pub async fn merge_definitions(url1: &str, url2: &str, url3: &str) -> Option<DictionaryEntry> {\n+pub async fn merge_definitions(url1: &str, url2: &str, url3: &str) -> Option<DictionaryError> {\n     let source_1 = oxford_scraper::download(url1).await;\n     let source_2 = oxford_scraper::download(url2).await;\n \n     if source_1.is_err() {\n"
                },
                {
                    "date": 1641795516879,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n use std::io::Cursor;\n \n \n use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType, DictionaryError, DictionaryErrorKind}, files::read_file_content, utils::trim_tabs};\n-use constants::USER_AGENT;\n+use constants::{USER_AGENT, INVALID};\n use itertools::Itertools;\n use log::{debug, error, info};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n"
                },
                {
                    "date": 1641795555225,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -237,9 +237,9 @@\n     let end = raw_element.find(end_with).unwrap() + end_with.len();\n     Some(raw_element[start..end].to_lowercase())\n }\n \n-pub async fn scrape_it_from(dictionary: String, word: &str) -> Option<DictionaryEntry> {\n+pub async fn scrape_it_from(dictionary: String, word: &str) -> Option<DictionaryError> {\n     match Dictionary::from(dictionary) {\n         Dictionary::Cambridge => cambridge_scraper::scrape(word).await,\n         Dictionary::Collins => collins_scraper::scrape(word).await,\n         Dictionary::Oxford => oxford_scraper::scrape(word).await,\n"
                },
                {
                    "date": 1641795567103,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -237,9 +237,9 @@\n     let end = raw_element.find(end_with).unwrap() + end_with.len();\n     Some(raw_element[start..end].to_lowercase())\n }\n \n-pub async fn scrape_it_from(dictionary: String, word: &str) -> Option<DictionaryError> {\n+pub async fn scrape_it_from(dictionary: String, word: &str) -> Option<DictionaryEntry> {\n     match Dictionary::from(dictionary) {\n         Dictionary::Cambridge => cambridge_scraper::scrape(word).await,\n         Dictionary::Collins => collins_scraper::scrape(word).await,\n         Dictionary::Oxford => oxford_scraper::scrape(word).await,\n"
                },
                {
                    "date": 1641795588133,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -172,9 +172,9 @@\n fn mp3_element(mp3_query: &str, html_content: &str) -> Option<String> {\n     first_element(mp3_query, html_content, false)\n }\n \n-pub async fn merge_definitions(url1: &str, url2: &str, url3: &str) -> Option<DictionaryError> {\n+pub async fn merge_definitions(url1: &str, url2: &str, url3: &str) -> Option<DictionaryEntry> {\n     let source_1 = oxford_scraper::download(url1).await;\n     let source_2 = oxford_scraper::download(url2).await;\n \n     if source_1.is_err() {\n"
                },
                {
                    "date": 1641795658391,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,8 +2,10 @@\n pub mod collins_scraper;\n pub mod oxford_scraper;\n pub mod unit_tests;\n pub mod constants;\n+pub mod model;\n+\n use std::io::Cursor;\n \n \n use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType, DictionaryError, DictionaryErrorKind}, files::read_file_content, utils::trim_tabs};\n"
                },
                {
                    "date": 1641795673191,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,8 +11,9 @@\n use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType, DictionaryError, DictionaryErrorKind}, files::read_file_content, utils::trim_tabs};\n use constants::{USER_AGENT, INVALID};\n use itertools::Itertools;\n use log::{debug, error, info};\n+use model::DictionaryEntry;\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n \n use crate::constants::NOT_FOUND;\n"
                },
                {
                    "date": 1641795680459,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType, DictionaryError, DictionaryErrorKind}, files::read_file_content, utils::trim_tabs};\n use constants::{USER_AGENT, INVALID};\n use itertools::Itertools;\n use log::{debug, error, info};\n-use model::DictionaryEntry;\n+use model::{DictionaryEntry, Dictionary};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n \n use crate::constants::NOT_FOUND;\n"
                },
                {
                    "date": 1641795689635,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n \n use std::io::Cursor;\n \n \n-use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType, DictionaryError, DictionaryErrorKind}, files::read_file_content, utils::trim_tabs};\n+use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType, DictionaryError, DictionaryErrorKind}, files::read_file_content, utils::{trim_tabs, replace_tabs}};\n use constants::{USER_AGENT, INVALID};\n use itertools::Itertools;\n use log::{debug, error, info};\n use model::{DictionaryEntry, Dictionary};\n"
                },
                {
                    "date": 1641795768419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n use std::io::Cursor;\n \n \n use common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType, DictionaryError, DictionaryErrorKind}, files::read_file_content, utils::{trim_tabs, replace_tabs}};\n-use constants::{USER_AGENT, INVALID};\n+use constants::{USER_AGENT, INVALID, MP3_EXT};\n use itertools::Itertools;\n use log::{debug, error, info};\n use model::{DictionaryEntry, Dictionary};\n use reqwest::StatusCode;\n"
                },
                {
                    "date": 1641823330552,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -248,4 +248,8 @@\n         Dictionary::Oxford => oxford_scraper::scrape(word).await,\n         _ => return None,\n     }\n }\n+\n+pub async fn <S: AsRef<str>> download(words: Vec<String>) {\n+\n+}\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823338520,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,7 +249,7 @@\n         _ => return None,\n     }\n }\n \n-pub async fn <S: AsRef<str>> download(words: Vec<String>) {\n+pub async <S: AsRef<str>> fn download(words: Vec<String>) {\n \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823369208,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,7 +249,7 @@\n         _ => return None,\n     }\n }\n \n-pub async <S: AsRef<str>> fn download(words: Vec<String>) {\n+pub async fn <S: AsRef<str>>  download(words: Vec<String>) {\n \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823411882,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,7 +249,7 @@\n         _ => return None,\n     }\n }\n \n-pub async fn <S: AsRef<str>>  download(words: Vec<String>) {\n+pub async fn   download<S: AsRef<str>> (words: Vec<String>) {\n \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823422504,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,7 +249,7 @@\n         _ => return None,\n     }\n }\n \n-pub async fn   download<S: AsRef<str>> (words: Vec<String>) {\n+pub async fn download<S: AsRef<str>> (words: Vec<S>) {\n \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823549574,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,7 +249,11 @@\n         _ => return None,\n     }\n }\n \n-pub async fn download<S: AsRef<str>> (words: Vec<S>) {\n-\n+pub fn download<S: AsRef<str>> (words: Vec<S>) {\n+    tokio::spawn(async move {\n+        for w in words {\n+            oxford_scraper::scrape(w);\n+        }\n+    });\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823559231,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -252,8 +252,8 @@\n \n pub fn download<S: AsRef<str>> (words: Vec<S>) {\n     tokio::spawn(async move {\n         for w in words {\n-            oxford_scraper::scrape(w);\n+            oxford_scraper::scrape(w).await;\n         }\n     });\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823568487,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -252,8 +252,8 @@\n \n pub fn download<S: AsRef<str>> (words: Vec<S>) {\n     tokio::spawn(async move {\n         for w in words {\n-            oxford_scraper::scrape(w).await;\n+            oxford_scraper::scrape(&w).await;\n         }\n     });\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823592125,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n }\n \n pub fn download<S: AsRef<str>> (words: Vec<S>) {\n     tokio::spawn(async move {\n-        for w in words {\n-            oxford_scraper::scrape(&w).await;\n+        for w in words.as_ref() {\n+            oxford_scraper::scrape(w).await;\n         }\n     });\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823652178,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,8 +251,9 @@\n }\n \n pub fn download<S: AsRef<str>> (words: Vec<S>) {\n     tokio::spawn(async move {\n+        let source = words.as_ref();\n         for w in words.as_ref() {\n             oxford_scraper::scrape(w).await;\n         }\n     });\n"
                },
                {
                    "date": 1641823668920,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,10 +251,9 @@\n }\n \n pub fn download<S: AsRef<str>> (words: Vec<S>) {\n     tokio::spawn(async move {\n-        let source = words.as_ref();\n-        for w in words.as_ref() {\n-            oxford_scraper::scrape(w).await;\n+        for w in words {\n+            oxford_scraper::scrape(w.as_ref()).await;\n         }\n     });\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823707181,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,9 @@\n     }\n }\n \n pub fn download<S: AsRef<str>> (words: Vec<S>) {\n-    tokio::spawn(async move {\n+    let _ = tokio::spawn(async move {\n         for w in words {\n             oxford_scraper::scrape(w.as_ref()).await;\n         }\n     });\n"
                },
                {
                    "date": 1641823750451,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,9 +249,9 @@\n         _ => return None,\n     }\n }\n \n-pub fn download<S: AsRef<str>> (words: Vec<S>) {\n+pub  async fn download<S: AsRef<str>> (words: Vec<S>) {\n     let _ = tokio::spawn(async move {\n         for w in words {\n             oxford_scraper::scrape(w.as_ref()).await;\n         }\n"
                },
                {
                    "date": 1641823766175,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,11 +249,11 @@\n         _ => return None,\n     }\n }\n \n-pub  async fn download<S: AsRef<str>> (words: Vec<S>) {\n-    let _ = tokio::spawn(async move {\n+pub async fn download<S: AsRef<str>> (words: Vec<S>) {\n+    \n         for w in words {\n             oxford_scraper::scrape(w.as_ref()).await;\n         }\n-    });\n+    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823774215,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,10 +250,9 @@\n     }\n }\n \n pub async fn download<S: AsRef<str>> (words: Vec<S>) {\n-    \n-        for w in words {\n+    for w in words {\n             oxford_scraper::scrape(w.as_ref()).await;\n-        }\n+    }\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823839545,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,10 +249,12 @@\n         _ => return None,\n     }\n }\n \n-pub async fn download<S: AsRef<str>> (words: Vec<S>) {\n-    for w in words {\n-            oxford_scraper::scrape(w.as_ref()).await;\n+pub async fn scrape_it_from(dictionary: String, word: &str) -> Option<DictionaryEntry> {\n+    match Dictionary::from(dictionary) {\n+        Dictionary::Cambridge => cambridge_scraper::scrape(word).await,\n+        Dictionary::Collins => collins_scraper::scrape(word).await,\n+        Dictionary::Oxford => oxford_scraper::scrape(word).await,\n+        _ => return None,\n     }\n-    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823847289,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,12 +249,4 @@\n         _ => return None,\n     }\n }\n \n-pub async fn scrape_it_from(dictionary: String, word: &str) -> Option<DictionaryEntry> {\n-    match Dictionary::from(dictionary) {\n-        Dictionary::Cambridge => cambridge_scraper::scrape(word).await,\n-        Dictionary::Collins => collins_scraper::scrape(word).await,\n-        Dictionary::Oxford => oxford_scraper::scrape(word).await,\n-        _ => return None,\n-    }\n-}\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823887689,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,4 +249,7 @@\n         _ => return None,\n     }\n }\n \n+pub fn scrape_all(sourece: Vec<String>) {\n+    \n+}\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823893636,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,7 +249,7 @@\n         _ => return None,\n     }\n }\n \n-pub fn scrape_all(sourece: Vec<String>) {\n+pub fn scrape_all(source: Vec<String>) {\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823914611,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,7 +249,7 @@\n         _ => return None,\n     }\n }\n \n-pub fn scrape_all(source: Vec<String>) {\n+pub fn scrape_all<S: AsRef<str>>(source: Vec<String>) {\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823932757,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -240,16 +240,16 @@\n     let end = raw_element.find(end_with).unwrap() + end_with.len();\n     Some(raw_element[start..end].to_lowercase())\n }\n \n-pub async fn scrape_it_from(dictionary: String, word: &str) -> Option<DictionaryEntry> {\n+pub async fn scrape_it_from<S: AsRef<str>>(dictionary: String, word: S) -> Option<DictionaryEntry> {\n     match Dictionary::from(dictionary) {\n         Dictionary::Cambridge => cambridge_scraper::scrape(word).await,\n         Dictionary::Collins => collins_scraper::scrape(word).await,\n         Dictionary::Oxford => oxford_scraper::scrape(word).await,\n         _ => return None,\n     }\n }\n \n-pub fn scrape_all<S: AsRef<str>>(source: Vec<String>) {\n+pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641823952857,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -242,11 +242,11 @@\n }\n \n pub async fn scrape_it_from<S: AsRef<str>>(dictionary: String, word: S) -> Option<DictionaryEntry> {\n     match Dictionary::from(dictionary) {\n-        Dictionary::Cambridge => cambridge_scraper::scrape(word).await,\n-        Dictionary::Collins => collins_scraper::scrape(word).await,\n-        Dictionary::Oxford => oxford_scraper::scrape(word).await,\n+        Dictionary::Cambridge => cambridge_scraper::scrape(word.as_ref()).await,\n+        Dictionary::Collins => collins_scraper::scrape(word.as_ref()).await,\n+        Dictionary::Oxford => oxford_scraper::scrape(word.as_ref()).await,\n         _ => return None,\n     }\n }\n \n"
                },
                {
                    "date": 1641824462967,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,6 +250,8 @@\n     }\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n-    \n+    for w in source {\n+        \n+    }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824513292,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,7 +251,7 @@\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n-        \n+        scrape_it_from(Dictionary::Cambridge, word);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824524244,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -252,6 +252,8 @@\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n         scrape_it_from(Dictionary::Cambridge, word);\n+        scrape_it_from(Dictionary::Collins, word);\n+        scrape_it_from(Dictionary::Oxford, word);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824565093,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n-        scrape_it_from(Dictionary::Cambridge, word);\n-        scrape_it_from(Dictionary::Collins, word);\n-        scrape_it_from(Dictionary::Oxford, word);\n+        scrape_it_from(\"Cambridge\", word);\n+        scrape_it_from(\"Collins\", word);\n+        scrape_it_from(\"Dictionary::\"Oxford\", word);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824571869,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -253,7 +253,7 @@\n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n         scrape_it_from(\"Cambridge\", word);\n         scrape_it_from(\"Collins\", word);\n-        scrape_it_from(\"Dictionary::\"Oxford\", word);\n+        scrape_it_from(\"Oxford\", word);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824593527,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -240,9 +240,9 @@\n     let end = raw_element.find(end_with).unwrap() + end_with.len();\n     Some(raw_element[start..end].to_lowercase())\n }\n \n-pub async fn scrape_it_from<S: AsRef<str>>(dictionary: String, word: S) -> Option<DictionaryEntry> {\n+pub async fn scrape_it_from<S: AsRef<str>>(dictionary: S, word: S) -> Option<DictionaryEntry> {\n     match Dictionary::from(dictionary) {\n         Dictionary::Cambridge => cambridge_scraper::scrape(word.as_ref()).await,\n         Dictionary::Collins => collins_scraper::scrape(word.as_ref()).await,\n         Dictionary::Oxford => oxford_scraper::scrape(word.as_ref()).await,\n"
                },
                {
                    "date": 1641824618059,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -240,9 +240,9 @@\n     let end = raw_element.find(end_with).unwrap() + end_with.len();\n     Some(raw_element[start..end].to_lowercase())\n }\n \n-pub async fn scrape_it_from<S: AsRef<str>>(dictionary: S, word: S) -> Option<DictionaryEntry> {\n+pub async fn scrape_it_from<S: AsRef<str>>(dictionary: String, word: S) -> Option<DictionaryEntry> {\n     match Dictionary::from(dictionary) {\n         Dictionary::Cambridge => cambridge_scraper::scrape(word.as_ref()).await,\n         Dictionary::Collins => collins_scraper::scrape(word.as_ref()).await,\n         Dictionary::Oxford => oxford_scraper::scrape(word.as_ref()).await,\n"
                },
                {
                    "date": 1641824639664,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n-        scrape_it_from(\"Cambridge\", word);\n-        scrape_it_from(\"Collins\", word);\n-        scrape_it_from(\"Oxford\", word);\n+        scrape_it_from(\"Cambridge\".to_lowercase(), word);\n+        scrape_it_from(\"Collins\".to_lowercase(), word);\n+        scrape_it_from(\"Oxford\".to_lowercase(), word);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824660192,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n-        scrape_it_from(\"Cambridge\".to_lowercase(), word);\n-        scrape_it_from(\"Collins\".to_lowercase(), word);\n-        scrape_it_from(\"Oxford\".to_lowercase(), word);\n+        scrape_it_from(\"Cambridge\".to_lowercase(), w);\n+        scrape_it_from(\"Collins\".to_lowercase(), w);\n+        scrape_it_from(\"Oxford\".to_lowercase(), w);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824670428,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n-        scrape_it_from(\"Cambridge\".to_lowercase(), w);\n+        scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref());\n         scrape_it_from(\"Collins\".to_lowercase(), w);\n         scrape_it_from(\"Oxford\".to_lowercase(), w);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824676936,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -252,8 +252,8 @@\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n         scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref());\n-        scrape_it_from(\"Collins\".to_lowercase(), w);\n-        scrape_it_from(\"Oxford\".to_lowercase(), w);\n+        scrape_it_from(\"Collins\".to_lowercase(), w.as_ref());\n+        scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref());\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824682624,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n-        scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref());\n-        scrape_it_from(\"Collins\".to_lowercase(), w.as_ref());\n-        scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref());\n+        scrape_it_from(\"Cambridge\".to_lowercase(), w);\n+        scrape_it_from(\"Collins\".to_lowercase(), w);\n+        scrape_it_from(\"Oxford\".to_lowercase(), w);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641824717086,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,9 +249,9 @@\n         _ => return None,\n     }\n }\n \n-pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n+pub fn scrape_all<S: AsRef<str> + Copy>(source: Vec<S>) {\n     for w in source {\n         scrape_it_from(\"Cambridge\".to_lowercase(), w);\n         scrape_it_from(\"Collins\".to_lowercase(), w);\n         scrape_it_from(\"Oxford\".to_lowercase(), w);\n"
                },
                {
                    "date": 1641824726580,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,9 +249,9 @@\n         _ => return None,\n     }\n }\n \n-pub fn scrape_all<S: AsRef<str> + Copy>(source: Vec<S>) {\n+pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n         scrape_it_from(\"Cambridge\".to_lowercase(), w);\n         scrape_it_from(\"Collins\".to_lowercase(), w);\n         scrape_it_from(\"Oxford\".to_lowercase(), w);\n"
                },
                {
                    "date": 1641829444718,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n-        scrape_it_from(\"Cambridge\".to_lowercase(), w);\n-        scrape_it_from(\"Collins\".to_lowercase(), w);\n-        scrape_it_from(\"Oxford\".to_lowercase(), w);\n+        scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n+        scrape_it_from(\"Collins\".to_lowercase(), &w);\n+        scrape_it_from(\"Oxford\".to_lowercase(), &w);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641829458523,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n-        scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n+        let _ = scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n         scrape_it_from(\"Collins\".to_lowercase(), &w);\n         scrape_it_from(\"Oxford\".to_lowercase(), &w);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641829464306,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -252,8 +252,8 @@\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n         let _ = scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n-        scrape_it_from(\"Collins\".to_lowercase(), &w);\n-        scrape_it_from(\"Oxford\".to_lowercase(), &w);\n+        let _ = scrape_it_from(\"Collins\".to_lowercase(), &w);\n+        let _ = scrape_it_from(\"Oxford\".to_lowercase(), &w);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641829501569,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     for w in source {\n-        let _ = scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n-        let _ = scrape_it_from(\"Collins\".to_lowercase(), &w);\n-        let _ = scrape_it_from(\"Oxford\".to_lowercase(), &w);\n+        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n+        let f2 = scrape_it_from(\"Collins\".to_lowercase(), &w);\n+        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), &w);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641829524342,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,8 +250,9 @@\n     }\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n+    let mut futures = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), &w);\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), &w);\n"
                },
                {
                    "date": 1641829538787,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -255,6 +255,9 @@\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), &w);\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), &w);\n+        futures.push(f1);\n+        futures.push(f2);\n+        futures.push(f3);\n     }\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641829582889,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -259,5 +259,6 @@\n         futures.push(f1);\n         futures.push(f2);\n         futures.push(f3);\n     }\n+    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641829722639,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,9 @@\n     }\n }\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n-    let mut futures = vec![];\n+    let mut tasks = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), &w);\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), &w);\n@@ -260,5 +260,6 @@\n         futures.push(f2);\n         futures.push(f3);\n     }\n     \n+    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641829738205,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -255,11 +255,11 @@\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), &w);\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), &w);\n-        futures.push(f1);\n-        futures.push(f2);\n-        futures.push(f3);\n+        tasks.push(f1);\n+        tasks.push(f2);\n+        tasks.push(f3);\n     }\n     \n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641829759982,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -260,6 +260,6 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n-    \n+    features::future::join_all(tasks);\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831079780,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,8 +15,9 @@\n use model::{DictionaryEntry, Dictionary};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n \n+\n use crate::constants::NOT_FOUND;\n \n \n pub fn wget(url: &str, file_name: &str) -> ServiceExuctionResult<String> {\n@@ -260,6 +261,6 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n-    features::future::join_all(tasks);\n+    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831136074,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,9 +15,9 @@\n use model::{DictionaryEntry, Dictionary};\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n \n-\n+use futures::future::join_all;\n use crate::constants::NOT_FOUND;\n \n \n pub fn wget(url: &str, file_name: &str) -> ServiceExuctionResult<String> {\n@@ -262,5 +262,6 @@\n         tasks.push(f3);\n     }\n     \n     \n+    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831227966,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -261,7 +261,7 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n+    join_all(tasks);\n     \n-    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831261391,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -253,11 +253,11 @@\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     let mut tasks = vec![];\n     for w in source {\n-        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), &w);\n-        let f2 = scrape_it_from(\"Collins\".to_lowercase(), &w);\n-        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), &w);\n+        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n+        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n+        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref().clone());\n         tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n"
                },
                {
                    "date": 1641831298812,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,9 @@\n         _ => return None,\n     }\n }\n \n-pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n+async pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     let mut tasks = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n"
                },
                {
                    "date": 1641831307025,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,9 @@\n         _ => return None,\n     }\n }\n \n-async pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n+pub async fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     let mut tasks = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n"
                },
                {
                    "date": 1641831372292,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,12 +250,12 @@\n         _ => return None,\n     }\n }\n \n-pub async fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n+pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     let mut tasks = vec![];\n     for w in source {\n-        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n+//        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref().clone());\n         tasks.push(f1);\n         tasks.push(f2);\n"
                },
                {
                    "date": 1641831382302,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -256,9 +256,9 @@\n     for w in source {\n //        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref().clone());\n-        tasks.push(f1);\n+  //      tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n"
                },
                {
                    "date": 1641831487398,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -253,15 +253,15 @@\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     let mut tasks = vec![];\n     for w in source {\n-//        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n+        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref().clone());\n-  //      tasks.push(f1);\n+        tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n-    join_all(tasks);\n+    join_all(move tasks);\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831493233,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -261,7 +261,7 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n-    join_all(move tasks);\n+    join_all(tasks);\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831508801,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -261,7 +261,6 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n-    join_all(tasks);\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831535074,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,9 @@\n         _ => return None,\n     }\n }\n \n-pub fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n+pub fn scrape_all<S: AsRef<str>>(source: Vec<String>) {\n     let mut tasks = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n"
                },
                {
                    "date": 1641831541493,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -253,9 +253,9 @@\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<String>) {\n     let mut tasks = vec![];\n     for w in source {\n-        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n+        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w().clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref().clone());\n         tasks.push(f1);\n         tasks.push(f2);\n"
                },
                {
                    "date": 1641831560013,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -253,11 +253,11 @@\n \n pub fn scrape_all<S: AsRef<str>>(source: Vec<String>) {\n     let mut tasks = vec![];\n     for w in source {\n-        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w().clone());\n-        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n-        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref().clone());\n+        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n+        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n+        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n"
                },
                {
                    "date": 1641831573836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -261,6 +261,7 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n+    join_all(tasks);\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831586247,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -261,7 +261,7 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n-    join_all(tasks);\n+    let _ = join_all(tasks);\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831594034,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -261,7 +261,7 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n-    let _ = join_all(tasks);\n+    let _ = join_all(tasks).await;\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641831602309,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,9 @@\n         _ => return None,\n     }\n }\n \n-pub fn scrape_all<S: AsRef<str>>(source: Vec<String>) {\n+pub  async fn scrape_all<S: AsRef<str>>(source: Vec<String>) {\n     let mut tasks = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n"
                },
                {
                    "date": 1641832905817,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -261,7 +261,7 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }\n     \n-    let _ = join_all(tasks).await;\n+    let _ = join_all(tasks);\n     \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641834617327,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -259,9 +259,7 @@\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n-    }\n-    \n-    let _ = join_all(tasks);\n-    \n+    }    \n+    let _ = join_all(tasks);    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1641886184713,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,8 +3,9 @@\n pub mod oxford_scraper;\n pub mod unit_tests;\n pub mod constants;\n pub mod model;\n+pub mod task_executor;\n \n use std::io::Cursor;\n \n \n"
                },
                {
                    "date": 1641978865558,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,9 +14,9 @@\n use itertools::Itertools;\n use log::{debug, error, info};\n use model::{DictionaryEntry, Dictionary};\n use reqwest::StatusCode;\n-use scraper::{Html, Selector};\n+use crate::scraper::{Html, Selector};\n \n use futures::future::join_all;\n use crate::constants::NOT_FOUND;\n \n"
                },
                {
                    "date": 1641978873452,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,9 +14,9 @@\n use itertools::Itertools;\n use log::{debug, error, info};\n use model::{DictionaryEntry, Dictionary};\n use reqwest::StatusCode;\n-use crate::scraper::{Html, Selector};\n+use scraper::{Html, Selector};\n \n use futures::future::join_all;\n use crate::constants::NOT_FOUND;\n \n"
                }
            ],
            "date": 1641556596513,
            "name": "Commit-0",
            "content": ""
        }
    ]
}