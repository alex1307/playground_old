{
    "sourceFile": "mk-scraper/src/lib.rs",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 32,
            "patches": [
                {
                    "date": 1641985170438,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1643881354605,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n         _ => return None,\n     }\n }\n \n-pub  async fn scrape_all<S: AsRef<str>>(source: Vec<String>) {\n+pub  async fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     let mut tasks = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n"
                },
                {
                    "date": 1643881367571,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -254,9 +254,9 @@\n \n pub  async fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     let mut tasks = vec![];\n     for w in source {\n-        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n+        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n         tasks.push(f2);\n"
                },
                {
                    "date": 1643881377027,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -255,10 +255,10 @@\n pub  async fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n     let mut tasks = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n-        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n-        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n+        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n+        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref().clone());\n         tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n     }    \n"
                },
                {
                    "date": 1643881392501,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,14 +251,14 @@\n         _ => return None,\n     }\n }\n \n-pub  async fn scrape_all<S: AsRef<str>>(source: Vec<S>) {\n+pub  async fn scrape_all(source: Vec<String>) {\n     let mut tasks = vec![];\n     for w in source {\n-        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.as_ref().clone());\n-        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.as_ref().clone());\n-        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.as_ref().clone());\n+        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n+        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n+        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n     }    \n"
                },
                {
                    "date": 1643883285046,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -252,8 +252,9 @@\n     }\n }\n \n pub  async fn scrape_all(source: Vec<String>) {\n+    info!(\"scrape_all is being executed.\");\n     let mut tasks = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n"
                },
                {
                    "date": 1643907562837,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -252,9 +252,9 @@\n     }\n }\n \n pub  async fn scrape_all(source: Vec<String>) {\n-    info!(\"scrape_all is being executed.\");\n+    info!(\"scrape_all is being executed for {:?}\", source);\n     let mut tasks = vec![];\n     for w in source {\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n"
                },
                {
                    "date": 1643907709999,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -243,8 +243,9 @@\n     Some(raw_element[start..end].to_lowercase())\n }\n \n pub async fn scrape_it_from<S: AsRef<str>>(dictionary: String, word: S) -> Option<DictionaryEntry> {\n+    info!(\"scrape from {}\", dictionary);\n     match Dictionary::from(dictionary) {\n         Dictionary::Cambridge => cambridge_scraper::scrape(word.as_ref()).await,\n         Dictionary::Collins => collins_scraper::scrape(word.as_ref()).await,\n         Dictionary::Oxford => oxford_scraper::scrape(word.as_ref()).await,\n"
                },
                {
                    "date": 1643907748271,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -264,5 +264,19 @@\n         tasks.push(f2);\n         tasks.push(f3);\n     }    \n     let _ = join_all(tasks);    \n+}\n+\n+pub  async fn tokio_scrape_all(source: Vec<String>) {\n+    info!(\"scrape_all is being executed for {:?}\", source);\n+    let mut tasks = vec![];\n+    for w in source {\n+        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n+        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n+        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n+        tasks.push(f1);\n+        tasks.push(f2);\n+        tasks.push(f3);\n+    }    \n+    let _ = join_all(tasks);    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1643907803774,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -270,13 +270,14 @@\n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     let mut tasks = vec![];\n     for w in source {\n+        tokio::spawn(async move {scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()})\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n     }    \n-    let _ = join_all(tasks);    \n+//    let _ = join_all(tasks);    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1643907810926,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -270,9 +270,9 @@\n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     let mut tasks = vec![];\n     for w in source {\n-        tokio::spawn(async move {scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()})\n+        tokio::spawn(async move {scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await});\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n"
                },
                {
                    "date": 1643907836746,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -270,9 +270,9 @@\n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     let mut tasks = vec![];\n     for w in source {\n-        tokio::spawn(async move {scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await});\n+        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await);\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n"
                },
                {
                    "date": 1643907843234,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -270,9 +270,9 @@\n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     let mut tasks = vec![];\n     for w in source {\n-        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await);\n+        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n"
                },
                {
                    "date": 1643907858547,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -270,9 +270,11 @@\n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     let mut tasks = vec![];\n     for w in source {\n-        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n+        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n+        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n+        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n         let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n         let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n"
                },
                {
                    "date": 1643907880385,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -270,14 +270,14 @@\n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     let mut tasks = vec![];\n     for w in source {\n-        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n-        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n-        tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n-        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n-        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n-        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n+        let f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n+        let f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n+        let f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n+        // let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n+        // let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n+        // let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n     }    \n"
                },
                {
                    "date": 1643907886573,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -280,6 +280,6 @@\n         tasks.push(f1);\n         tasks.push(f2);\n         tasks.push(f3);\n     }    \n-//    let _ = join_all(tasks);    \n+    let _ = join_all(tasks);    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1643908077832,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -270,9 +270,9 @@\n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     let mut tasks = vec![];\n     for w in source {\n-        let f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n+        let f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n         let f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n         // let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         // let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n"
                },
                {
                    "date": 1643908108457,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -270,16 +270,16 @@\n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     let mut tasks = vec![];\n     for w in source {\n-        let f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n-        let f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n-        let f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()));\n+        let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n+        let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n+        let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         // let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n         // let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n-        tasks.push(f1);\n-        tasks.push(f2);\n-        tasks.push(f3);\n+        // tasks.push(f1);\n+        // tasks.push(f2);\n+        // tasks.push(f3);\n     }    \n-    let _ = join_all(tasks);    \n+    // let _ = join_all(tasks);    \n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1643908114315,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -268,9 +268,9 @@\n }\n \n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n-    let mut tasks = vec![];\n+    // let mut tasks = vec![];\n     for w in source {\n         let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n"
                },
                {
                    "date": 1643908210044,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -269,8 +269,13 @@\n \n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     // let mut tasks = vec![];\n+    let runtime = runtime::Runtime::new().unwrap();\n+    runtime.block_on(async move {\n+        let _value = tokio::spawn(tokio_scrape_all(request.words)).await.unwrap();\n+\n+    });\n     for w in source {\n         let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n"
                },
                {
                    "date": 1643908215763,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,8 +17,9 @@\n use reqwest::StatusCode;\n use scraper::{Html, Selector};\n \n use futures::future::join_all;\n+use tokio::runtime;\n use crate::constants::NOT_FOUND;\n \n \n pub fn wget(url: &str, file_name: &str) -> ServiceExuctionResult<String> {\n"
                },
                {
                    "date": 1643908249042,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -271,13 +271,14 @@\n pub  async fn tokio_scrape_all(source: Vec<String>) {\n     info!(\"scrape_all is being executed for {:?}\", source);\n     // let mut tasks = vec![];\n     let runtime = runtime::Runtime::new().unwrap();\n-    runtime.block_on(async move {\n-        let _value = tokio::spawn(tokio_scrape_all(request.words)).await.unwrap();\n-\n-    });\n+    \n     for w in source {\n+        runtime.block_on(async move {\n+            let _value = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await.unwrap();\n+    \n+        });\n         let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n"
                },
                {
                    "date": 1643908273895,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -274,9 +274,9 @@\n     let runtime = runtime::Runtime::new().unwrap();\n     \n     for w in source {\n         runtime.block_on(async move {\n-            let _value = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await.unwrap();\n+            tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await;\n     \n         });\n         let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n"
                },
                {
                    "date": 1643908308975,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -275,9 +275,8 @@\n     \n     for w in source {\n         runtime.block_on(async move {\n             tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await;\n-    \n         });\n         let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n"
                },
                {
                    "date": 1643908360013,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -274,9 +274,9 @@\n     let runtime = runtime::Runtime::new().unwrap();\n     \n     for w in source {\n         runtime.block_on(async move {\n-            tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await;\n+            tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await.unwrap();\n         });\n         let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n"
                },
                {
                    "date": 1643908379776,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -276,11 +276,11 @@\n     for w in source {\n         runtime.block_on(async move {\n             tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await.unwrap();\n         });\n-        let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n-        let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n-        let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n+        // let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n+        // let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n+        // let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n         // let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n         // let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n         // tasks.push(f1);\n"
                },
                {
                    "date": 1643908393956,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -274,9 +274,9 @@\n     let runtime = runtime::Runtime::new().unwrap();\n     \n     for w in source {\n         runtime.block_on(async move {\n-            tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone()).await.unwrap();\n+            tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await.unwrap();\n         });\n         // let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n"
                },
                {
                    "date": 1643908456750,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -276,8 +276,15 @@\n     for w in source {\n         runtime.block_on(async move {\n             tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await.unwrap();\n         });\n+        runtime.block_on(async move {\n+            tokio::spawn(scrape_it_from(\"Collins\".to_lowercase(), w.clone())).await.unwrap();\n+        });\n+\n+        runtime.block_on(async move {\n+            tokio::spawn(scrape_it_from(\"Oxford\".to_lowercase(), w.clone())).await.unwrap();\n+        });\n         // let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n"
                },
                {
                    "date": 1643908495019,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -280,11 +280,11 @@\n         runtime.block_on(async move {\n             tokio::spawn(scrape_it_from(\"Collins\".to_lowercase(), w.clone())).await.unwrap();\n         });\n \n-        runtime.block_on(async move {\n-            tokio::spawn(scrape_it_from(\"Oxford\".to_lowercase(), w.clone())).await.unwrap();\n-        });\n+        // runtime.block_on(async move {\n+        //     tokio::spawn(scrape_it_from(\"Oxford\".to_lowercase(), w.clone())).await.unwrap();\n+        // });\n         // let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n"
                },
                {
                    "date": 1643908510686,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -272,9 +272,9 @@\n     info!(\"scrape_all is being executed for {:?}\", source);\n     // let mut tasks = vec![];\n     let runtime = runtime::Runtime::new().unwrap();\n     \n-    for w in source {\n+    for &w in source {\n         runtime.block_on(async move {\n             tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await.unwrap();\n         });\n         runtime.block_on(async move {\n"
                },
                {
                    "date": 1643908533649,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -272,11 +272,12 @@\n     info!(\"scrape_all is being executed for {:?}\", source);\n     // let mut tasks = vec![];\n     let runtime = runtime::Runtime::new().unwrap();\n     \n-    for &w in source {\n+    for w in source {\n+        let nw = w.clone();\n         runtime.block_on(async move {\n-            tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await.unwrap();\n+            tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), nw.clone())).await.unwrap();\n         });\n         runtime.block_on(async move {\n             tokio::spawn(scrape_it_from(\"Collins\".to_lowercase(), w.clone())).await.unwrap();\n         });\n"
                },
                {
                    "date": 1643908550710,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,10 +277,11 @@\n         let nw = w.clone();\n         runtime.block_on(async move {\n             tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), nw.clone())).await.unwrap();\n         });\n+        let nw = w.clone();\n         runtime.block_on(async move {\n-            tokio::spawn(scrape_it_from(\"Collins\".to_lowercase(), w.clone())).await.unwrap();\n+            tokio::spawn(scrape_it_from(\"Collins\".to_lowercase(), nw.clone())).await.unwrap();\n         });\n \n         // runtime.block_on(async move {\n         //     tokio::spawn(scrape_it_from(\"Oxford\".to_lowercase(), w.clone())).await.unwrap();\n"
                },
                {
                    "date": 1643908556549,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -282,11 +282,11 @@\n         runtime.block_on(async move {\n             tokio::spawn(scrape_it_from(\"Collins\".to_lowercase(), nw.clone())).await.unwrap();\n         });\n \n-        // runtime.block_on(async move {\n-        //     tokio::spawn(scrape_it_from(\"Oxford\".to_lowercase(), w.clone())).await.unwrap();\n-        // });\n+        runtime.block_on(async move {\n+            tokio::spawn(scrape_it_from(\"Oxford\".to_lowercase(), w.clone())).await.unwrap();\n+        });\n         // let _f1 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let _f2 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let _f3 = tokio::spawn(scrape_it_from(\"Cambridge\".to_lowercase(), w.clone())).await;\n         // let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n"
                }
            ],
            "date": 1641985170438,
            "name": "Commit-0",
            "content": "pub mod cambridge_scraper;\npub mod collins_scraper;\npub mod oxford_scraper;\npub mod unit_tests;\npub mod constants;\npub mod model;\npub mod task_executor;\n\nuse std::io::Cursor;\n\n\nuse common_libs::{error::{ServiceExuctionResult, ServiceError, ServiceErrorType, DictionaryError, DictionaryErrorKind}, files::read_file_content, utils::{trim_tabs, replace_tabs}};\nuse constants::{USER_AGENT, INVALID, MP3_EXT};\nuse itertools::Itertools;\nuse log::{debug, error, info};\nuse model::{DictionaryEntry, Dictionary};\nuse reqwest::StatusCode;\nuse scraper::{Html, Selector};\n\nuse futures::future::join_all;\nuse crate::constants::NOT_FOUND;\n\n\npub fn wget(url: &str, file_name: &str) -> ServiceExuctionResult<String> {\n    let status = std::process::Command::new(\"wget\")\n    .arg(\"--user-agent\")\n    .arg(USER_AGENT)\n    .arg(url)\n    .arg(\"-O\")\n    .arg(file_name)\n    .status();\n\n    if status.is_err() {\n        error!(\"failed.executing.wget: {}\", status.err().unwrap());\n        return Err(ServiceError {\n            message: \"wget.failed\".to_string(),\n            error_type: ServiceErrorType::Failure,\n        });\n    }\n\n    let bytes = read_file_content(file_name)?;\n    Ok(String::from_utf8(bytes)?)\n}\n\npub async fn fetch_url(url: String, file_name: String) -> ServiceExuctionResult<()> {\n    debug!(\"downloading from [{}] to {}\", url, file_name);\n    let response = reqwest::get(url.clone()).await.unwrap();\n    let status = response.status();\n    info!(\"download status: {}\", status);\n    if status.is_success() {\n        let mut file = std::fs::File::create(file_name.clone())?;\n        let mut content = Cursor::new(response.bytes().await?);\n        std::io::copy(&mut content, &mut file)?;\n        Ok(())\n    } else {\n        match wget(&url, &file_name) {\n            Ok(_) => Ok(()),\n            Err(err) => Err(err)\n        }\n    }\n    \n}\n\nasync fn download_from_url(url: &str) -> ServiceExuctionResult<String> {\n    println!(\"URL: {}\", url.clone());\n    let response = reqwest::get(url).await?;\n    println!(\"resposne: {:?}\", response);\n    if StatusCode::OK != response.status() {\n        let msg = format!(\"nok.status.code: {}\", response.status().as_str());\n        return Err(ServiceError {\n            message: msg,\n            error_type: ServiceErrorType::Unavailable,\n        });\n    }\n\n    let bytes = response.bytes().await.unwrap();\n    let slice = &bytes[..];\n    let content = String::from_utf8(Vec::from(slice)).unwrap();\n    let lower = content.to_lowercase();\n    for i in 0..NOT_FOUND.len() {\n        if lower.contains(NOT_FOUND[i]) {\n            return Err(ServiceError {\n                message: \"not.found\".to_string(),\n                error_type: ServiceErrorType::ResourceNotFound,\n            });\n        }\n    }\n    Ok(content)\n}\n\nfn description(inner_html: String) -> Option<String> {\n    let mut replaced = match inner_text(inner_html.clone(), \"a\") {\n        Ok(inner) => inner,\n        Err(_err) => return None,\n    };\n\n    replaced = match inner_text(replaced, \"span\") {\n        Ok(inner) => inner,\n        Err(_) => return None,\n    };\n\n    replaced = replaced.replace(\"\\n\", \" \");\n    replaced = trim_tabs(&mut replaced);\n    replaced = replaced.replace(\" .\", \".\");\n    Some(replaced)\n}\n\nfn inner_text(inner_html: String, tag: &str) -> Result<String, DictionaryError> {\n    let mut replaced = inner_html.clone();\n    let mut open_tag = String::from(\"<\");\n    open_tag.push_str(tag);\n    let mut close_tag = String::from(\"</\");\n    close_tag.push_str(tag);\n    close_tag.push_str(\">\");\n\n    if !(inner_html.contains(&open_tag) && inner_html.contains(&close_tag)) {\n        return Ok(inner_html);\n    }\n\n    let tag_selector = Selector::parse(tag).unwrap();\n\n    while replaced.contains(&open_tag) && replaced.contains(&close_tag) {\n        let start = replaced.find(&open_tag).unwrap();\n        let end = replaced.find(&close_tag).unwrap();\n        if start < end + close_tag.len() {\n            let to_be_replaced = &replaced[start..end + close_tag.len()];\n            let a_href = Html::parse_fragment(&replaced);\n            let inner = a_href.select(&tag_selector).next().unwrap().inner_html();\n            replaced = replaced.replace(to_be_replaced, &inner);\n        } else {\n            error!(\n                \"parse.error: start postion {} is bofore end: {}\",\n                start,\n                end + close_tag.len()\n            );\n            return Err(DictionaryError::throw(\n                \"unable.to.parse.definition\",\n                DictionaryErrorKind::InvalidData,\n            ));\n        }\n    }\n    Ok(replaced)\n}\n\nfn elements(query: &str, html_content: &str, inner: bool) -> Vec<String> {\n    let fragment = Html::parse_fragment(&html_content);\n    let selector = Selector::parse(query).unwrap();\n    let elements = fragment.select(&selector).into_iter();\n    let mut all = Vec::new();\n    for e in elements {\n        if inner {\n            all.push(e.inner_html());\n        } else {\n            all.push(e.html());\n        }\n    }\n    all\n}\n\nfn first_element(query: &str, html_content: &str, inner: bool) -> Option<String> {\n    let fragment = Html::parse_fragment(&html_content);\n    let selector = Selector::parse(query).unwrap();\n    let mut elements = fragment.select(&selector).into_iter();\n    let found = elements.next();\n\n    if found.is_some() {\n        return if inner {\n            Some(found.unwrap().inner_html())\n        } else {\n            Some(found.unwrap().html())\n        };\n    }\n\n    None\n}\n\nfn mp3_element(mp3_query: &str, html_content: &str) -> Option<String> {\n    first_element(mp3_query, html_content, false)\n}\n\npub async fn merge_definitions(url1: &str, url2: &str, url3: &str) -> Option<DictionaryEntry> {\n    let source_1 = oxford_scraper::download(url1).await;\n    let source_2 = oxford_scraper::download(url2).await;\n\n    if source_1.is_err() {\n        return None;\n    }\n\n    if source_2.is_err() {\n        return None;\n    }\n\n    let mut merged = source_1.unwrap();\n    for def in source_2.unwrap().definitions {\n        merged.definitions.push(def);\n    }\n    if !url3.is_empty() {\n        let source_3 = oxford_scraper::download(url3).await;\n        if source_3.is_ok() {\n            for def in source_3.unwrap().definitions {\n                merged.definitions.push(def);\n            }\n        }\n    }\n\n    Some(merged)\n}\n\nfn to_url(base_url: &str, word: &str) -> String {\n    if word.is_empty() {\n        return INVALID.to_lowercase();\n    }\n\n    if base_url.is_empty() {\n        return INVALID.to_lowercase();\n    }\n    let mut w = String::from(word);\n    let mut trimmed = trim_tabs(&mut w);\n    let processed = replace_tabs(&mut trimmed, \"-\");\n    let mut url = String::from(base_url);\n    url.push_str(&processed);\n    url.to_lowercase()\n}\n\nfn mp3_element_to_url(source: &str, start_with: &str) -> Option<String> {\n    element_to_url(source, start_with, MP3_EXT)\n}\n\nfn element_to_url(source: &str, start_with: &str, end_with: &str) -> Option<String> {\n    let values: Vec<&str> = source.split_whitespace().collect();\n    let found = values\n        .into_iter()\n        .find_or_first(|s| s.contains(start_with) && s.contains(end_with));\n\n    let raw_element = if found.is_some() {\n        found.unwrap()\n    } else {\n        return None;\n    };\n\n    let start = raw_element.find(start_with).unwrap();\n    let end = raw_element.find(end_with).unwrap() + end_with.len();\n    Some(raw_element[start..end].to_lowercase())\n}\n\npub async fn scrape_it_from<S: AsRef<str>>(dictionary: String, word: S) -> Option<DictionaryEntry> {\n    match Dictionary::from(dictionary) {\n        Dictionary::Cambridge => cambridge_scraper::scrape(word.as_ref()).await,\n        Dictionary::Collins => collins_scraper::scrape(word.as_ref()).await,\n        Dictionary::Oxford => oxford_scraper::scrape(word.as_ref()).await,\n        _ => return None,\n    }\n}\n\npub  async fn scrape_all<S: AsRef<str>>(source: Vec<String>) {\n    let mut tasks = vec![];\n    for w in source {\n        let f1 = scrape_it_from(\"Cambridge\".to_lowercase(), w.clone());\n        let f2 = scrape_it_from(\"Collins\".to_lowercase(), w.clone());\n        let f3 = scrape_it_from(\"Oxford\".to_lowercase(), w.clone());\n        tasks.push(f1);\n        tasks.push(f2);\n        tasks.push(f3);\n    }    \n    let _ = join_all(tasks);    \n}"
        }
    ]
}